{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detectron2 Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Detectron2 Beginner's Tutorial\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of detectron2, including the following:\n",
        "* Run inference on images or videos, with an existing detectron2 model\n",
        "* Train a detectron2 model on a new dataset\n",
        "\n",
        "You can make a copy of this tutorial by \"File -> Open in playground mode\" and make changes there. __DO NOT__ request access to this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "# Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JxxLubvU4VA",
        "outputId": "8d7017dc-5e98-4381-c987-b9924624c9c0"
      },
      "source": [
        "!nvidia-smi\n",
        "!nvcc -V"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun 24 14:49:24 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH78B0LUU9cl",
        "outputId": "d50fdb5b-6e91-412a-92ac-7f07996f196f"
      },
      "source": [
        "!python -c \"import torch; print(torch.__version__); import torchvision; print(torchvision.__version__)\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n",
            "0.10.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FzH13EjseR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6762a9-931e-4390-c34e-4ec98df07b3d"
      },
      "source": [
        "# install dependencies: \n",
        "!pip install pyyaml==5.1\n",
        "!pip install cmake\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyyaml==5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 13.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 17.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 19.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 12.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44091 sha256=7585ff72b39befcb850d4c8e79d6171f3e04eb28b3cb92921f4b48631879b2e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.12.0)\n",
            "1.9.0+cu102 True\n",
            "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "Copyright (C) 2017 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNn9jbBgRR0C",
        "outputId": "2625e44a-b0a3-468a-c40d-dc654887c180"
      },
      "source": [
        "# Manually install torch 1.8.1 as bug in 1.8.0\n",
        "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip install torchtext==0.9.1\n",
        "\n",
        "#exit(0) # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cu111\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1MB 1.8MB/s eta 0:10:52tcmalloc: large alloc 1147494400 bytes == 0x55b1e60c2000 @  0x7f48637ce615 0x55b1ad7c3cdc 0x55b1ad8a352a 0x55b1ad7c6afd 0x55b1ad8b7fed 0x55b1ad83a988 0x55b1ad8354ae 0x55b1ad7c83ea 0x55b1ad83a7f0 0x55b1ad8354ae 0x55b1ad7c83ea 0x55b1ad83732a 0x55b1ad8b8e36 0x55b1ad836853 0x55b1ad8b8e36 0x55b1ad836853 0x55b1ad8b8e36 0x55b1ad836853 0x55b1ad8b8e36 0x55b1ad93b3e1 0x55b1ad89b6a9 0x55b1ad806cc4 0x55b1ad7c7559 0x55b1ad83b4f8 0x55b1ad7c830a 0x55b1ad8363b5 0x55b1ad8357ad 0x55b1ad7c83ea 0x55b1ad8363b5 0x55b1ad7c830a 0x55b1ad8363b5\n",
            "\u001b[K     |█████████████████               | 1055.7MB 44.8MB/s eta 0:00:21tcmalloc: large alloc 1434370048 bytes == 0x55b22a718000 @  0x7f48637ce615 0x55b1ad7c3cdc 0x55b1ad8a352a 0x55b1ad7c6afd 0x55b1ad8b7fed 0x55b1ad83a988 0x55b1ad8354ae 0x55b1ad7c83ea 0x55b1ad83a7f0 0x55b1ad8354ae 0x55b1ad7c83ea 0x55b1ad83732a 0x55b1ad8b8e36 0x55b1ad836853 0x55b1ad8b8e36 0x55b1ad836853 0x55b1ad8b8e36 0x55b1ad836853 0x55b1ad8b8e36 0x55b1ad93b3e1 0x55b1ad89b6a9 0x55b1ad806cc4 0x55b1ad7c7559 0x55b1ad83b4f8 0x55b1ad7c830a 0x55b1ad8363b5 0x55b1ad8357ad 0x55b1ad7c83ea 0x55b1ad8363b5 0x55b1ad7c830a 0x55b1ad8363b5\n",
            "\u001b[K     |█████████████████████▋          | 1336.2MB 1.7MB/s eta 0:06:10tcmalloc: large alloc 1792966656 bytes == 0x55b1af54a000 @  0x7f48637ce615 0x55b1ad7c3cdc 0x55b1ad8a352a 0x55b1ad7c6afd 0x55b1ad8b7fed 0x55b1ad83a988 0x55b1ad8354ae 0x55b1ad7c83ea 0x55b1ad83a7f0 0x55b1ad8354ae 0x55b1ad7c83ea 0x55b1ad83732a 0x55b1ad8b8e36 0x55b1ad836853 0x55b1ad8b8e36 0x55b1ad836853 0x55b1ad8b8e36 0x55b1ad836853 0x55b1ad8b8e36 0x55b1ad93b3e1 0x55b1ad89b6a9 0x55b1ad806cc4 0x55b1ad7c7559 0x55b1ad83b4f8 0x55b1ad7c830a 0x55b1ad8363b5 0x55b1ad8357ad 0x55b1ad7c83ea 0x55b1ad8363b5 0x55b1ad7c830a 0x55b1ad8363b5\n",
            "\u001b[K"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzoPUAH9iRxX"
      },
      "source": [
        "print(torch.__version__, torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzfYza-jmZSU"
      },
      "source": [
        "!nvcc --version\n",
        "!ls -l /usr/local | grep cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU1MbLcupxZh"
      },
      "source": [
        "!cat /usr/local/cuda/version.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4hmGYk1dL"
      },
      "source": [
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.8)\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "import torch\n",
        "assert torch.__version__.startswith(\"1.8.\")   # need to manually install torch 1.8 if Colab changes its default version\n",
        "!python -m pip install detectron2==0.4 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "#exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeJp986zrkVS"
      },
      "source": [
        "!python -c 'import torch;print(torch.eye(3))'\n",
        "!python -c 'from torch.utils.collect_env import main; main()'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk4gID50K03a"
      },
      "source": [
        "# Run a pre-trained detectron2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgKyUL4pngvE"
      },
      "source": [
        "We first download an image from the COCO dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq9GY37ml1kr"
      },
      "source": [
        "!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
        "im = cv2.imread(\"./input.jpg\")\n",
        "cv2_imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM1thbN-ntjI"
      },
      "source": [
        "Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUjkwRsOn1O0"
      },
      "source": [
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d3KxiHO_0gb"
      },
      "source": [
        "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "print(outputs[\"instances\"].pred_classes)\n",
        "print(outputs[\"instances\"].pred_boxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IRGo8d0qkgR"
      },
      "source": [
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo"
      },
      "source": [
        "# Train on a custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjbUIhSxUdm_"
      },
      "source": [
        "In this section, we show how to train an existing detectron2 model on a custom dataset in a new format.\n",
        "\n",
        "We use [the balloon segmentation dataset](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)\n",
        "which only has one class: balloon.\n",
        "We'll train a balloon segmentation model from an existing model pre-trained on COCO dataset, available in detectron2's model zoo.\n",
        "\n",
        "Note that COCO dataset does not have the \"balloon\" category. We'll be able to recognize this new class in a few minutes.\n",
        "\n",
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qg7zSVOulkb"
      },
      "source": [
        "# download, decompress the data\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n",
        "!unzip balloon_dataset.zip > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVJoOm6LVJwW"
      },
      "source": [
        "Register the balloon dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n",
        "Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format. User should write such a function when using a dataset in custom format. See the tutorial for more details.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIbAM2pv-urF"
      },
      "source": [
        "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
        "# from detectron2.data.datasets import register_coco_instances\n",
        "# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path/to/image/dir\")\n",
        "# register_coco_instances(\"my_dataset_val\", {}, \"json_annotation_val.json\", \"path/to/image/dir\")\n",
        "\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "def get_balloon_dicts(img_dir):\n",
        "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
        "    with open(json_file) as f:\n",
        "        imgs_anns = json.load(f)\n",
        "\n",
        "    dataset_dicts = []\n",
        "    for idx, v in enumerate(imgs_anns.values()):\n",
        "        record = {}\n",
        "        \n",
        "        filename = os.path.join(img_dir, v[\"filename\"])\n",
        "        height, width = cv2.imread(filename).shape[:2]\n",
        "        \n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"image_id\"] = idx\n",
        "        record[\"height\"] = height\n",
        "        record[\"width\"] = width\n",
        "      \n",
        "        annos = v[\"regions\"]\n",
        "        objs = []\n",
        "        for _, anno in annos.items():\n",
        "            assert not anno[\"region_attributes\"]\n",
        "            anno = anno[\"shape_attributes\"]\n",
        "            px = anno[\"all_points_x\"]\n",
        "            py = anno[\"all_points_y\"]\n",
        "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": 0,\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "for d in [\"train\", \"val\"]:\n",
        "    DatasetCatalog.register(\"balloon_\" + d, lambda d=d: get_balloon_dicts(\"balloon/\" + d))\n",
        "    MetadataCatalog.get(\"balloon_\" + d).set(thing_classes=[\"balloon\"])\n",
        "balloon_metadata = MetadataCatalog.get(\"balloon_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ljbWTX0Wi8E"
      },
      "source": [
        "To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkNbUzUOLYf0"
      },
      "source": [
        "dataset_dicts = get_balloon_dicts(\"balloon/train\")\n",
        "\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA"
      },
      "source": [
        "## Train!\n",
        "\n",
        "Now, let's fine-tune a COCO-pretrained R50-FPN Mask R-CNN model on the balloon dataset. It takes ~6 minutes to train 300 iterations on Colab's K80 GPU, or ~2 minutes on a P100 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7unkuuiqLdqd"
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"balloon_train\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBXeH8UXFcqU"
      },
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4vdDIOXyxF"
      },
      "source": [
        "## Inference & evaluation using the trained model\n",
        "Now, let's run inference with the trained model on the balloon validation dataset. First, let's create a predictor using the model we just trained:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya5nEuMELeq8"
      },
      "source": [
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWq1XHfDWiXO"
      },
      "source": [
        "Then, we randomly select several samples to visualize the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5LhISJqWXgM"
      },
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "dataset_dicts = get_balloon_dicts(\"balloon/val\")\n",
        "for d in random.sample(dataset_dicts, 3):    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=balloon_metadata, \n",
        "                   scale=0.5, \n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kblA1IyFvWbT"
      },
      "source": [
        "We can also evaluate its performance using AP metric implemented in COCO API.\n",
        "This gives an AP of ~70. Not bad!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9tECBQCvMv3"
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"balloon_val\", (\"bbox\", \"segm\"), False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"balloon_val\")\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
        "# another equivalent way to evaluate the model is to use `trainer.test`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKBbjnLw5GGG"
      },
      "source": [
        "# Other types of builtin models\n",
        "\n",
        "We showcase simple demos of other types of models below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYJrlXZC5M-J"
      },
      "source": [
        "# Inference with a keypoint detection model\n",
        "cfg = get_cfg()   # get a fresh new config\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(im)\n",
        "v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roTj1N9F5uJ5"
      },
      "source": [
        "# Inference with a panoptic segmentation model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "panoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXadAb9Fv-L"
      },
      "source": [
        "# Run panoptic segmentation on a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU5_W8wJF02F"
      },
      "source": [
        "# This is the video we're going to process\n",
        "from IPython.display import YouTubeVideo, display\n",
        "video = YouTubeVideo(\"ll8TgCZ0plk\", width=500)\n",
        "display(video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a65jM_VFF2Hr"
      },
      "source": [
        "# Install dependencies, download the video, and crop 5 seconds for processing\n",
        "!pip install youtube-dl\n",
        "!youtube-dl https://www.youtube.com/watch?v=ll8TgCZ0plk -f 22 -o video.mp4\n",
        "!ffmpeg -i video.mp4 -t 00:00:06 -c:v copy video-clip.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyA4VmKcF61k"
      },
      "source": [
        "# Run frame-by-frame inference demo on this video (takes 3-4 minutes) with the \"demo.py\" tool we provided in the repo.\n",
        "!git clone https://github.com/facebookresearch/detectron2\n",
        "# Note: this is currently BROKEN due to missing codec. See https://github.com/facebookresearch/detectron2/issues/2901 for workaround.\n",
        "%run detectron2/demo/demo.py --config-file detectron2/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output video-output.mkv --opts MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpLg_MAQGPUT"
      },
      "source": [
        "# Download the results\n",
        "from google.colab import files\n",
        "files.download('video-output.mkv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQbadqxLncgj"
      },
      "source": [
        "## Experiment: FineTuning PubLayNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrm0xaEDnat9"
      },
      "source": [
        "# download, decompress the data\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1NAgGrYoQJwdNXE-nFzx0yEJEoqxMcqh2' -O siemens.zip # exchange\n",
        "!unzip siemens.zip -d datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlyxbGLmoDTD"
      },
      "source": [
        "### Register Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhnaAo5HoCgg"
      },
      "source": [
        "from detectron2.data.datasets import load_coco_json, register_coco_instances\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "cur_dir = os.getcwd()\n",
        "data_dir = os.path.join(cur_dir, \"datasets/siemens\")\n",
        "\n",
        "# Training dataset\n",
        "training_dataset_name = \"dla_train\"\n",
        "training_json_file = os.path.join(data_dir, \"train/train2021.json\")\n",
        "training_img_dir = os.path.join(data_dir, \"train/images\")\n",
        "\n",
        "# Register custom training dataset\n",
        "register_coco_instances(training_dataset_name, {}, training_json_file, training_img_dir)\n",
        "\n",
        "# Validation dataset\n",
        "validation_dataset_name = \"dla_val\"\n",
        "validation_json_file = os.path.join(data_dir, \"val/val2021.json\")\n",
        "validation_img_dir = os.path.join(data_dir, \"val/images\")\n",
        "    \n",
        "# Register custom training dataset\n",
        "register_coco_instances(validation_dataset_name, {}, validation_json_file, validation_img_dir)\n",
        "    \n",
        "# Get training/validation dictionary\n",
        "training_dict = load_coco_json(training_json_file, training_img_dir, dataset_name=training_dataset_name)\n",
        "validation_dict = load_coco_json(validation_json_file, validation_img_dir, dataset_name=validation_dataset_name)\n",
        "\n",
        "# Register metadata\n",
        "training_metadata = MetadataCatalog.get(training_dataset_name)\n",
        "validation_metadata = MetadataCatalog.get(validation_dataset_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a63aQoDVrK4o"
      },
      "source": [
        "### Get pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D1SkayvrNmM"
      },
      "source": [
        "# download pre-trained model\n",
        "!mkdir -p pretrained_models\n",
        "!wget https://www.dropbox.com/sh/wgt9skz67usliei/AADGw0h1y7K5vO0akulyXm-qa/model_final.pth?dl=0 -P /dev/null/pretrained_models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PakqhRNorz-b"
      },
      "source": [
        "### Train the pre-trained Mask-RCNN R50 FPN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USiRrEOory0r"
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "def setup(config_file, training_metadata):\n",
        "    \"\"\"\n",
        "    Create configs and perform basic setups.\n",
        "    \"\"\"\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(config_file)\n",
        "    cfg.DATASETS.TRAIN = (\"dla_train\",)\n",
        "    cfg.DATASETS.TEST = (\"val_train\")\n",
        "    cfg.DATALOADER.NUM_WORKERS = 2\n",
        "    cfg.MODEL.WEIGHTS = \"pretrained_models/Resnet-101/model_final.pth\"\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "    cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "    cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for a toy dataset; you will need to train longer for a practical dataset\n",
        "    cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # 128 faster, and good enough for a toy dataset (default: 512)\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(training_metadata.thing_classes)  # has 5 classes (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "    \n",
        "    # Solver options\n",
        "    cfg.SOLVER.BASE_LR = 1e-3           # Base learning rate\n",
        "    cfg.SOLVER.GAMMA = 0.5              # Learning rate decay\n",
        "    cfg.SOLVER.STEPS = (250, 500, 750)  # Iterations at which to decay learning rate\n",
        "    cfg.SOLVER.MAX_ITER = 1000          # Maximum number of iterations\n",
        "    cfg.SOLVER.WARMUP_ITERS = 100       # Warmup iterations to linearly ramp learning rate from zero\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 1        # Lower to reduce memory usage (1 is the lowest)\n",
        "    return cfg\n",
        "\n",
        "cfg = setup(config_file=\"DLA_mask_rcnn_R_101_FPN_3x.yaml\", training_metadata=training_metadata)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Start training\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Rr7gqlwdMi"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxsCvb59wcVa"
      },
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kw3MFmqwrkk"
      },
      "source": [
        "### Evaluation of model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bBaXpnSwq97"
      },
      "source": [
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzktdKGqw_Jx"
      },
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "dataset_dicts = get_balloon_dicts(\"siemens_dataset/val\")\n",
        "for d in random.sample(dataset_dicts, 3):    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=balloon_metadata, \n",
        "                   scale=0.5, \n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YSIMIIHxEgM"
      },
      "source": [
        "### Check metrics for COCO API "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCn_NwLsxEAc"
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"dla_val\", (\"bbox\", \"segm\"), False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"dla_val\")\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
        "# another equivalent way to evaluate the model is to use `trainer.test`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-yhFoxvurco"
      },
      "source": [
        "### Download fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HunigT5tuqty"
      },
      "source": [
        "files.download('fine_tunes_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}